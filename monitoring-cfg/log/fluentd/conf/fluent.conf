# ================================================================
# Input: Read logs from Docker containers
# ================================================================
<source>
  @type tail
  @id in_tail_docker_containers
  # More specific path for Docker's json-file log driver
  path /var/lib/docker/containers/*/*-json.log
  pos_file /fluentd/log/docker-containers.pos
  tag docker.*
  read_from_head true

  # Parse each line as a separate JSON object
  <parse>
    @type json
    # Use the timestamp from the Docker log entry itself
    time_key time
    time_format %Y-%m-%dT%H:%M:%S.%NZ
  </parse>
</source>

<filter docker.**>
  @type parser
  @id filter_docker_log_parser
  key_name log
  reserve_data true
  remove_key_name_field true
  emit_invalid_record_to_error false
  <parse>
    @type json
  </parse>
</filter>

# ================================================================
# Output: Forward logs to Elasticsearch
# ================================================================
<match **>
  @type elasticsearch
  @id out_elasticsearch
  host "#{ENV['FLUENT_ELASTICSEARCH_HOST']}"
  port "#{ENV['FLUENT_ELASTICSEARCH_PORT']}"
  user "#{ENV['FLUENT_ELASTICSEARCH_USER']}"
  password "#{ENV['FLUENT_ELASTICSEARCH_PASSWORD']}"

  # Modern Elasticsearch settings
  logstash_format true
  logstash_prefix docker-logs
  logstash_dateformat %Y.%m.%d
  type_name _doc # Use _doc for modern Elasticsearch versions

  # Robust buffer configuration to prevent data loss
  <buffer>
    @type file
    path /fluentd/buffer/es
    flush_mode interval
    flush_interval 5s
    flush_thread_count 2
    retry_type exponential_backoff
    retry_wait 1s
    retry_max_interval 60s
    retry_forever true
    chunk_limit_size 16M
    queue_limit_length 16
    overflow_action block
  </buffer>
</match>


#<match **>
#@type stdout
#</match>